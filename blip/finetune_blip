from torch.utils.data import DataLoader
from transformers import AutoModelForVision2Seq, AutoProcessor
from peft import LoraConfig, get_peft_model
import torch
import torch.optim as optim

import numpy as np
import os 
from PIL import Image
import pandas as pd
from torch.utils.data import Dataset
from typing import Literal
from torchvision import transforms



class ReceipesDataset(Dataset):
	def __init__(
			self,
			data_path: str,
			transform_image: bool = False,
			split: Literal["train", "val", "test"] = "train",
			split_size: list = [0.7, 0.1, 0.2],
			data_size: int=1.0
	):
		super(ReceipesDataset, self).__init__()
		self.img_path = os.path.join(data_path, 'FoodImages', 'Food Images')
		self.cap_path = os.path.join(data_path, 'FoodIngredientsAndReceipesDatasetWithImageNameMapping.csv')
		self.cap_data = pd.read_csv(self.cap_path)
		self.transform_image = transform_image
		self.split = split

		# Clean data
		self.cap_data = self.cap_data.dropna(subset=["Title"]) # TODO: Add other columns
		self.cap_data = self.cap_data[self.cap_data["Title"].apply(lambda x: len(x.split()) > 0)]
		self.cap_data = self.cap_data[self.cap_data["Image_Name"].apply(lambda x: x != "#NAME?")]

		# Create vocabulary
		# self.vocab = set()
		# for caption in self.cap_data["Title"]: # TODO: Add other columns
		# 	self.vocab.update(caption.split())
		# self.special_tokens = ['<start>', '<end>', '<pad>', '<unk>']
		# self.vocab.update(self.special_tokens)
		# self.vocab = sorted(self.vocab)
		# self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}
		# self.idx2word = {idx: word for word, idx in self.word2idx.items()}
		# self.vocab_size = len(self.vocab)

		# Split data
		total_size = len(self.cap_data)
		train_end = int(split_size[0] * total_size)
		val_end = train_end + int(split_size[1] * total_size)

		if split == "train":
			self.cap_data = self.cap_data[:train_end]
		elif split == "val":
			self.cap_data = self.cap_data[train_end:val_end]
		elif split == "test":
			self.cap_data = self.cap_data[val_end:]

		self.cap_data = self.cap_data.sample(frac=data_size, random_state=42)
	
	def __len__(self):
		return len(self.cap_data)
	
	def verify_paths(self):
		for idx in range(len(self)):
			img_name = os.path.join(self.img_path, self.cap_data.iloc[idx, 4]) + ".jpg"
			if not os.path.exists(img_name):
				print(f"Image {img_name} does not exist.")
	
	def __getitem__(self, idx):
		img_name = os.path.join(self.img_path, self.cap_data.iloc[idx, 4]) + ".jpg"
		image = Image.open(img_name).convert('RGB')
		image = np.array(image)[..., ::-1]
		image = Image.fromarray(image)
		if self.transform_image:
			image = transform(image)
		caption = self.cap_data.iloc[idx, 1]
		return image, caption


# Cargar el modelo BLIP
model_id = "Salesforce/blip-image-captioning-base"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForVision2Seq.from_pretrained(model_id)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

# Configuración PEFT
config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=[
        "self.query", "self.key", "self.value", 
        "output.dense", "self_attn.qkv", 
        "self_attn.projection", "mlp.fc1", "mlp.fc2"
    ],
)

model = get_peft_model(model, config)
model.print_trainable_parameters()


transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

data_path = "/path/to/your/dataset"
train_dataset = ReceipesDataset(data_path=data_path, transform_image=True, split="train")

# Collate Function para BLIP
def collator(batch):
    images, captions = zip(*batch)
    # Procesar imágenes
    image_inputs = processor(images=list(images), return_tensors="pt", padding=True)
    # Procesar texto
    text_inputs = processor.tokenizer(
        list(captions), padding=True, return_tensors="pt"
    )
    # Combinar todo
    return {
        "pixel_values": image_inputs["pixel_values"].to(device),
        "input_ids": text_inputs["input_ids"].to(device),
        "attention_mask": text_inputs["attention_mask"].to(device),
    }

# DataLoader
train_dataloader = DataLoader(
    train_dataset, shuffle=True, batch_size=4, collate_fn=collator
)

# Optimización
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)

# Entrenamiento
model.train()

for epoch in range(10):  # Ajusta el número de épocas
    print(f"Epoch {epoch + 1}")
    for idx, batch in enumerate(train_dataloader):
        input_ids = batch["input_ids"]
        pixel_values = batch["pixel_values"]
        attention_mask = batch["attention_mask"]

        outputs = model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            labels=input_ids,
            attention_mask=attention_mask,
        )

        loss = outputs.loss
        print(f"Batch {idx}, Loss: {loss.item()}")

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if idx % 10 == 0:
            # Generar texto de prueba
            generated_output = model.generate(
                pixel_values=pixel_values, max_new_tokens=64
            )
            print(processor.batch_decode(generated_output, skip_special_tokens=True))

# Guardar el modelo
model.save_pretrained("./training/recipes")
